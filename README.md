# Hairstyle Transfer between Face Images (FG 2021)



![Hairstyle Transfer](https://user-images.githubusercontent.com/52527235/175562680-d5c57b0e-6227-467f-b9be-908caba69202.png)


We propose a neural network which takes two inputs, a hair image and a face image, and produces an output image having the hair of the hair image seamlessly merged with the inner face of the face image. Our architecture consists of neural networks mapping the input images into a latent code of a pretrained StyleGAN2 which generates the output high-definition image. We propose an algorithm for training parameters of the architecture solely from synthetic images generated by the StyleGAN2 itself without the need of any annotations or external dataset of hairstyle images. We empirically demonstrate the effectiveness of our method in applications including hair-style transfer, hair generation for 3D morphable models, and hair-style interpolation. Fidelity of the generated images is verified by a user study and by a novel hairstyle metric proposed in the paper.

>Hairstyle Transfer between Face Images (FG 2021)</br>
Adéla Šubrtová, Jan Čech, Vojtěch Franc



[Paper](https://ieeexplore.ieee.org/abstract/document/9667038), [Project page](http://cmp.felk.cvut.cz/hairstyles/index), [Demo](http://cmp.felk.cvut.cz/hairstyles/demo) 




## Installation

Clone this repository.
```
git clone https://github.com/subrtade662/hairstyle_transfer.git
cd hairstyle_transfer
```

Install dependencies
```
pip install -r requirements.txt
```

Download required models:
  * [Pretrained network](https://drive.google.com/file/d/1gsJuFf2b927AqVp3BrlZkHv8wrk9gBbF/view?usp=sharing)
  * [Shape predictor](http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2)
  * [Hairstyle latent space directions for manipulation](https://drive.google.com/file/d/12RXDNfvqIQxARD_zl-MQLlT6lUkgnzYH/view?usp=sharing)


## Inference

### Hairstyle Transfer
```
python inference.py --output_path /path/to/save/the/results/ \
--mode transfer \
--source /path/to/img1.jpg \
--target /path/to/img2.png \
[--alpha_blend]
```

### Interpolation
```
python inference.py --output_path /path/to/save/the/results/ \
--mode interp \
--source /path/to/img1.jpg \
--target /path/to/img2.png  \
--nsteps 10 \
[--interpolate_face] \
[--alpha_blend]
```

### Manipulation
```
python inference.py --output_path /path/to/save/the/results/ \
--mode manip \
--source /path/to/img1.jpg  \
--attribute color/structure \
--strength 10,10.5,11. \
[--alpha_blend]
```

## Acknowledgements

Source code builds upon the work of Richardson et al. [pixel2style2pixel](https://github.com/eladrich/pixel2style2pixel).

## Citation
If you find this useful, please, cite our paper.

```
@INPROCEEDINGS{9667038,
  author={Šubrtová, Adéla and Čech, Jan and Franc, Vojtěch},
  booktitle={2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)}, 
  title={Hairstyle Transfer between Face Images}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/FG52635.2021.9667038}}

``` 

